{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from random import random\nfrom numpy import load\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy import asarray\nfrom numpy.random import randint\nfrom keras.optimizers import Adam\nfrom keras.initializers import RandomNormal\nfrom keras.models import Model\nfrom tensorflow.keras import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import Conv2DTranspose\nfrom keras.layers import LeakyReLU\nfrom keras.layers import Activation\nfrom keras.layers import Concatenate\nimport matplotlib.pyplot as plt\nfrom os import listdir\n#from keras.preprocessing.image import load_img\nfrom numpy import savez_compressed\nfrom keras import backend as K\nimport os\nimport numpy as np\nfrom PIL import Image\nimport pandas as pd\nimport tensorflow as tf\n\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n    \nprint(tf.__version__)\n\nwith strategy.scope():\n    normalized_pixel_values2 = pd.read_pickle('/kaggle/input/normalized1/normalized_pixel_values/npv2_data.pkl')\n\n    normalized_pixel_values1 = pd.read_pickle('/kaggle/input/normalized1/npv1_data.pkl/npv1_data.pkl')\n    \n    normalized_pixel_values3 = pd.read_pickle('/kaggle/input/normalized1/normalized_pixel_values/npv3_data.pkl')\n        \n    normalized_pixel_values4 = pd.read_pickle('/kaggle/input/normalized1/normalized_pixel_values/npv4_data.pkl')\n        \n\n        \nimport tensorflow as tf\nfrom tensorflow.keras.layers import GroupNormalization\n# define layer\nlayer = GroupNormalization(axis=-1, groups = -1)\nprint('import successful')\n\n    # Define your model here\n    # ...\n\ndef discriminator_model(image_shape):\n        init = RandomNormal(stddev = 0.02) #weight initialization with a normal distirbution curve\n        input_image = Input(shape = image_shape)\n        d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(input_image) #64 - number of layers/output channels, (4,4) - size of filter, padding = \"same\" - ensures output has same dimensions as input after padding\n        d = LeakyReLU(alpha=0.2)(d)         \n\n        d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n        d = GroupNormalization(axis=-1, groups = 1)(d)\n        d = LeakyReLU(alpha=0.2)(d)\n\n        d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n        d = GroupNormalization(axis=-1, groups = 1)(d)\n        d = LeakyReLU(alpha=0.2)(d)\n\n        d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n        d = GroupNormalization(axis=-1, groups = 1)(d)\n        d = LeakyReLU(alpha=0.2)(d)\n\n        d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n        d = GroupNormalization(axis=-1, groups = 1)(d)\n        d = LeakyReLU(alpha=0.2)(d)\n\n        patch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n        model = Model(input_image, patch_out)\n\n        model.compile(loss='mse', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), loss_weights=[0.5]) #lr - learning rate, beta_1 - influences moving average of past gradients, loss_weights - shows relative contribution of mse to the overall loss during training\n        print('discriminator model ready')\n        return model\n\ndef generator_model(image_shape):\n        init = RandomNormal(stddev = 0.02) #weight initialization with a normal distirbution curve\n        input_image = Input(shape = image_shape)\n        g = Conv2D(64, (7,7), padding = 'same', kernel_initializer = init)(input_image) #64 - number of filters, (7 x 7) - size of filter\n        g = GroupNormalization(axis=-1, groups = 1)(g)\n        g = Activation('relu')(g)\n\n        g =Conv2D(128, (3,3), strides = (2,2), padding = 'same', kernel_initializer = init)(g)\n        g = GroupNormalization(axis=-1, groups = 1)(g)\n        g = Activation('relu')(g)\n\n        g =Conv2D(256, (3,3), strides = (2,2), padding = 'same', kernel_initializer = init)(g)\n        g = GroupNormalization(axis=-1, groups = 1)(g)\n        g = Activation('relu')(g)\n\n        g = Conv2DTranspose(128, (3,3), strides = (2,2), padding = 'same', kernel_initializer = init)(g)\n        g = GroupNormalization(axis=-1, groups = 1)(g)\n        g = Activation('relu')(g)\n\n        g = Conv2DTranspose(64, (3,3), strides = (2,2), padding = 'same', kernel_initializer = init)(g)\n        g = GroupNormalization(axis=-1, groups = 1)(g)\n        g = Activation('relu')(g)\n\n        g = Conv2DTranspose(3, (7,7), padding = 'same', kernel_initializer = init)(g)\n        g = GroupNormalization(axis=-1, groups = 1)(g)\n        output_image = Activation('tanh')(g)\n        \n        model = Model(input_image, output_image)\n        print('generator model ready')\n        return model\n\ndef composite_model(g_model_1, d_model, g_model_2, image_shape):\n        g_model_1.trainable = True #we need to change the weights of the main generator\n        d_model.trainable = False\n        g_model_2.trainable = False\n\n        #discriminator element\n        input_gen = Input(shape = image_shape)\n        gen1_out = g_model_1(input_gen)\n        output_d = d_model(gen1_out)\n\n        #identity element\n        input_id = Input(shape = image_shape)\n        output_id = g_model_1(input_id)\n\n        #forward cycle\n        output_f = g_model_2(gen1_out)\n\n        #backward cycle\n        gen2_out = g_model_2(input_id)\n        output_b = g_model_1(gen2_out)\n\n        model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n        optimizer = Adam(learning_rate= 0.0002, beta_1 = 0.5)\n\n        model.compile(loss = ['mse', 'mae', 'mae', 'mae'], loss_weights = [1,5,10,10], optimizer = optimizer)\n        print('composite model ready')\n        return model \n\n           \ndef generate_real_samples(dataset, n_samples, patch_shape):\n         # choose random instances\n        ix = randint(0, dataset.shape[0], n_samples)\n         # retrieve selected images\n        X = dataset[ix]\n         # generate 'real' class labels (1)\n        y = ones((n_samples, patch_shape, patch_shape, 1))\n        print('real sample model ready')\n        return X, y\n\n    \ndef generate_fake_samples(g_model, dataset, patch_shape):\n     # generate fake instance\n        X = g_model.predict(dataset)\n     # create 'fake' class labels (0)\n        y = zeros((len(X), patch_shape, patch_shape, 1))\n        print('fake sample model ready')\n        return X, y\n\nwith strategy.scope():\n    dataset = [np.concatenate((normalized_pixel_values1, normalized_pixel_values2)), np.concatenate((normalized_pixel_values3, normalized_pixel_values4))]\n    dataM, dataP = dataset\n    image_shape = (256, 256, 3)\n    g_model_MtoP = generator_model(image_shape)\n    g_model_PtoM = generator_model(image_shape)\n    d_model_M = discriminator_model(image_shape)\n    d_model_P = discriminator_model(image_shape)\n    c_model_MtoP = composite_model(g_model_MtoP, d_model_P, g_model_PtoM, image_shape)\n    c_model_PtoM = composite_model(g_model_PtoM, d_model_M, g_model_MtoP, image_shape)\nprint('ready')\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\n\n    # Define hyperparameters\nnum_epochs = 2  # Adjust as needed\nbatch_size = 1024     # You can adjust the batch size\npatch_shape = d_model_M.output_shape[1]   # Adjust the patch size as needed\n\n    # Define a directory to save model checkpoints\ncheckpoint_dir = '/kaggle/working/model_checkpoints'  # Adjust the directory as needed\n\n    # Create the directory if it doesn't exist\nimport os\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n    # Define a file naming pattern for saved models\ncheckpoint_filepath = os.path.join(checkpoint_dir, 'model_epoch_{epoch:03d}.h5')\n\n    # Create a ModelCheckpoint callback to save models\nmodel_checkpoint_callback = ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,  # Save only the weights, not the entire model\n    monitor='val_loss',     # Monitor a specific metric (e.g., validation loss)\n    save_best_only=False,   # Save all models or just the best one\n    verbose=1, # Provide progress updates\n      \n    )\n    \n    # Training loop\n    # Training loop\ntraining_history = {'D_loss_realM': [], 'D_loss_realP': [], 'D_loss_fakeM': [], 'D_loss_fakeP': [], 'G_lossM': [], 'G_lossP': []}  # Initialize an empty dictionary to store the training history\n\nbat_per_epo = int(len(dataM) / batch_size)\n # calculate the number of training iterations\nn_steps = bat_per_epo * num_epochs\n # manually enumerate epochs\nfor epoch in range(n_steps):\n            # Train Discriminators\n    real_imagesM, real_labelsM = generate_real_samples(dataM, batch_size, patch_shape) #This line generates a batch of real images from the current batch in the dataset and assigns them real labels (usually 1, indicating real images).\n    real_imagesP, real_labelsP = generate_real_samples(dataP, batch_size, patch_shape)\n            \n    fake_imagesM, fake_labelsM = generate_fake_samples(g_model_PtoM, real_imagesP, patch_shape) #: This line generates a batch of fake images by passing a batch from the dataset through the generator g_model_MtoP and assigns them fake labels (usually 0, indicating fake images).\n    fake_imagesP, fake_labelsP = generate_fake_samples(g_model_MtoP, real_imagesM, patch_shape)\n            \n    g_lossP = c_model_PtoM.train_on_batch([real_imagesP, real_imagesM], [real_labelsM, real_imagesM, real_imagesP, real_imagesM])\n\n    d_loss_realM = d_model_M.train_on_batch(real_imagesM, real_labelsM)\n    d_loss_fakeM = d_model_M.train_on_batch(fake_imagesM, fake_labelsM)\n            \n    g_lossM = c_model_MtoP.train_on_batch([real_imagesM, real_imagesP], [real_labelsP, real_imagesP, real_imagesM, real_imagesP])\n\n    d_loss_realP = d_model_P.train_on_batch(real_imagesP, real_labelsP)\n    d_loss_fakeP = d_model_P.train_on_batch(fake_imagesP, fake_labelsP)\n\n    # Print training progress\n    print(f\"Epoch {epoch+1}/{n_steps}, D Loss RealM: {d_loss_realM}, D Loss RealP: {d_loss_realP}, D Loss FakeM: {d_loss_fakeM}, D Loss FakeP: {d_loss_fakeP}, G LossM: {g_lossM}, G LossP: {g_lossP}\")\n    training_history['D_loss_realM'].append(d_loss_realM)\n    training_history['D_loss_realP'].append(d_loss_realP)\n    training_history['D_loss_fakeM'].append(d_loss_fakeM)\n    training_history['D_loss_fakeP'].append(d_loss_fakeP)\n    training_history['G_lossM'].append(g_lossM)\n    training_history['G_lossM'].append(g_lossP)\n         \n\n\ng_model_MtoP.save('/kaggle/working/generator_model_MtoP.h5')\n# Save the generator model (you can do the same for the discriminator)\ng_model_PtoM.save('/kaggle/working/generator_model_PtoM.h5')\n# Save the generator model (you can do the same for the discriminator)\nd_model_M.save('/kaggle/working/discriminator_model_M.h5')\n# Save the generator model (you can do the same for the discriminator)\nd_model_P.save('/kaggle/working/discriminator_model_P.h5')\n\n# Save training history (optional)\nimport pickle\nwith open('/kaggle/working/training_history.pkl', 'wb') as history_file:\n    pickle.dump(training_history, history_file)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-11T13:14:20.324295Z","iopub.execute_input":"2023-09-11T13:14:20.324713Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"D0911 13:14:52.465048704      15 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\nD0911 13:14:52.465076971      15 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\nD0911 13:14:52.465080327      15 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\nD0911 13:14:52.465082925      15 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\nD0911 13:14:52.465085651      15 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\nD0911 13:14:52.465089076      15 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\nD0911 13:14:52.465091581      15 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\nD0911 13:14:52.465094006      15 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\nD0911 13:14:52.465096475      15 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\nD0911 13:14:52.465098788      15 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\nD0911 13:14:52.465101156      15 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\nD0911 13:14:52.465104580      15 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\nD0911 13:14:52.465107088      15 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\nD0911 13:14:52.465109341      15 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\nI0911 13:14:52.465299787      15 ev_epoll1_linux.cc:122]               grpc epoll fd: 65\nD0911 13:14:52.465311395      15 ev_posix.cc:144]                      Using polling engine: epoll1\nD0911 13:14:52.465330729      15 dns_resolver_ares.cc:822]             Using ares dns resolver\nD0911 13:14:52.465817229      15 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD0911 13:14:52.465829700      15 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD0911 13:14:52.465833674      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD0911 13:14:52.465836579      15 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD0911 13:14:52.465839896      15 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD0911 13:14:52.465842816      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\nD0911 13:14:52.465849401      15 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD0911 13:14:52.465866865      15 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD0911 13:14:52.465893597      15 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD0911 13:14:52.465907527      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD0911 13:14:52.465910707      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD0911 13:14:52.465913977      15 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD0911 13:14:52.465920124      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\nD0911 13:14:52.465923302      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD0911 13:14:52.465926378      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD0911 13:14:52.465930287      15 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\nI0911 13:14:52.468885056      15 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\nI0911 13:14:52.482094802     381 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE0911 13:14:52.489312600     381 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2023-09-11T13:14:52.489296866+00:00\", grpc_status:2}\n","output_type":"stream"},{"name":"stdout","text":"Device: \nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\nINFO:tensorflow:Finished initializing TPU system.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Found TPU system:\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Found TPU system:\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Cores: 8\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Cores: 8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Workers: 1\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Workers: 1\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"Number of replicas: 8\n2.12.0\nimport successful\ndiscriminator model ready\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.8/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"generator model ready\ngenerator model ready\ndiscriminator model ready\ndiscriminator model ready\ncomposite model ready\ncomposite model ready\nready\nreal sample model ready\nreal sample model ready\n","output_type":"stream"},{"name":"stderr","text":"2023-09-11 13:16:56.360422: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2023-09-11 13:16:56.390303: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"32/32 [==============================] - 26s 34ms/step\nfake sample model ready\n","output_type":"stream"},{"name":"stderr","text":"2023-09-11 13:17:24.529260: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2023-09-11 13:17:24.562605: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"32/32 [==============================] - 26s 33ms/step\nfake sample model ready\n","output_type":"stream"},{"name":"stderr","text":"2023-09-11 13:18:15.920169: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/Adam/AssignAddVariableOp.\n2023-09-11 13:18:16.194075: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/Adam/AssignAddVariableOp.\n2023-09-11 13:19:00.732614: E tensorflow/core/tpu/kernels/tpu_compilation_cache_external.cc:113] XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 35.76G of 15.48G hbm. Exceeded hbm capacity by 20.27G.\n\nTotal hbm usage >= 36.27G:\n    reserved        530.00M \n    program          35.76G \n    arguments            0B \n\nOutput size 0B; shares 0B with arguments.\n\nProgram hbm requirement 35.76G:\n    global            36.0K\n    HLO temp         35.76G (99.4% utilization: Unpadded (34.61G) Padded (34.82G), 2.6% fragmentation (962.99M))\n\n  Largest program allocations in hbm:\n\n  1. Size: 2.00G\n     Operator: op_type=\"Conv2D\" op_name=\"model_5/model_1/conv2d_3/Conv2D_1\"\n     Shape: f32[128,256,256,64]{0,3,2,1:T(8,128)}\n     Unpadded size: 2.00G\n     XLA label: fusion.63.remat3 = fusion(get-tuple-element.1088, get-tuple-element.916, get-tuple-element.842, get-tuple-element.841, ...(+1)), kind=kOutput, calls=fused_computation.61.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  2. Size: 2.00G\n     Operator: op_type=\"Conv2D\" op_name=\"model_5/model_1/conv2d_3/Conv2D_2\"\n     Shape: f32[128,256,256,64]{0,3,2,1:T(8,128)}\n     Unpadded size: 2.00G\n     XLA label: fusion.67.remat3 = fusion(copy.2.remat, get-tuple-element.1088, get-tuple-element.1089), kind=kOutput, calls=fused_computation.65.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  3. Size: 2.00G\n     Operator: op_type=\"Conv2DBackpropInput\" op_name=\"model_5/model_1/conv2d_transpose_4/conv2d_transpose\"\n     Shape: f32[128,256,256,64]{0,3,2,1:T(8,128)}\n     Unpadded size: 2.00G\n     XLA label: fusion.57.remat3.1.remat3 = fusion(get-tuple-element.1097, get-tuple-element.1096, fusion.227.remat2.1.remat, get-tuple-element.819, ...(+1)), kind=kOutput, calls=fused_computation.55.clone.clone.clone.1.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  4. Size: 2.00G\n     Operator: op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/model_5/model_1/conv2d_4/Conv2D_1/Conv2DBackpropInput\"\n     Shape: f32[128,256,256,64]{0,3,2,1:T(8,128)}\n     Unpadded size: 2.00G\n     XLA label: fusion.1249 = fusion(fusion.63.remat3, get-tuple-element.1091, fusion.222.remat3, get-tuple-element.924, ...(+6)), kind=kOutput, calls=fused_computation.1095\n     Allocation type: HLO temp\n     ==========================\n\n  5. Size: 2.00G\n     Operator: op_type=\"Conv2D\" op_name=\"gradient_tape/model_5/model_1/conv2d_transpose_5/conv2d_transpose_2/Conv2D\"\n     Shape: f32[128,256,256,64]{0,3,2,1:T(8,128)}\n     Unpadded size: 2.00G\n     XLA label: fusion.1252.remat3 = fusion(get-tuple-element.1246, get-tuple-element.1099, get-tuple-element.952, fusion.847, ...(+7)), kind=kOutput, calls=fused_computation.1098.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  6. Size: 2.00G\n     Operator: op_type=\"Conv2D\" op_name=\"gradient_tape/model_5/model_1/conv2d_transpose_5/conv2d_transpose_1/Conv2D\"\n     Shape: f32[128,256,256,64]{0,3,2,1:T(8,128)}\n     Unpadded size: 2.00G\n     XLA label: fusion.1250 = fusion(fusion.54.remat, get-tuple-element.1099, get-tuple-element.948, fusion.865, ...(+7)), kind=kOutput, calls=fused_computation.1096\n     Allocation type: HLO temp\n     ==========================\n\n  7. Size: 2.00G\n     Operator: op_type=\"Conv2D\" op_name=\"model_5/model_1/conv2d_3/Conv2D\"\n     Shape: f32[128,256,256,64]{0,3,2,1:T(8,128)}\n     Unpadded size: 2.00G\n     XLA label: fusion.66.remat4 = fusion(copy.6.remat2.1, get-tuple-element.1088, get-tuple-element.1089), kind=kOutput, calls=fused_computation.64.clone.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  8. Size: 2.00G\n     Operator: op_type=\"Conv2DBackpropInput\" op_name=\"model_5/model_1/conv2d_transpose_4/conv2d_transpose_2\"\n     Shape: f32[128,256,256,64]{0,3,2,1:T(8,128)}\n     Unpadded size: 2.00G\n     XLA label: fusion.1369.remat3 = fusion(get-tuple-element.1096, get-tuple-element.1097, get-tuple-element.1241, get-tuple-element.821, ...(+1)), kind=kOutput, calls=fused_computation.1143.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  9. Size: 2.00G\n     Operator: op_type=\"Conv2DBackpropInput\" op_name=\"model_5/model_1/conv2d_transpose_4/conv2d_transpose_1\"\n     Shape: f32[128,256,256,64]{0,3,2,1:T(8,128)}\n     Unpadded size: 2.00G\n     XLA label: fusion.54.remat2.1.remat2 = fusion(get-tuple-element.1097, get-tuple-element.1096, get-tuple-element.1243, get-tuple-element.818, ...(+1)), kind=kOutput, calls=fused_computation.52.clone.clone.1.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  10. Size: 2.00G\n     Operator: op_type=\"Conv2DBackpropInput\" op_name=\"model_5/model_1/conv2d_transpose_4/conv2d_transpose\"\n     Shape: f32[128,256,256,64]{0,3,2,1:T(8,128)}\n     Unpadded size: 2.00G\n     XLA label: fusion.57.remat7 = fusion(get-tuple-element.1097, get-tuple-element.1096, fusion.227.remat2.1.remat, get-tuple-element.819, ...(+1)), kind=kOutput, calls=fused_computation.55.clone.clone.clone.clone.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  11. Size: 2.00G\n     Operator: op_type=\"Conv2D\" op_name=\"gradient_tape/model_5/model_1/conv2d_transpose_5/conv2d_transpose/Conv2D\"\n     Shape: f32[128,256,256,64]{0,3,2,1:T(8,128)}\n     Unpadded size: 2.00G\n     XLA label: fusion.1246.remat = fusion(fusion.57.remat3.1.remat.1, get-tuple-element.1099, get-tuple-element.984, get-tuple-element.950, ...(+6)), kind=kOutput, calls=fused_computation.1092.clone\n     Allocation type: HLO temp\n     ==========================\n\n  12. Size: 1.00G\n     Operator: op_type=\"Conv2D\" op_name=\"model_5/model_1/conv2d_4/Conv2D\"\n     Shape: f32[128,128,128,128]{3,0,2,1:T(8,128)}\n     Unpadded size: 1.00G\n     XLA label: fusion.1359 = fusion(get-tuple-element.1090, get-tuple-element.1091, get-tuple-element.920, get-tuple-element.837, ...(+1)), kind=kOutput, calls=fused_computation.1133\n     Allocation type: HLO temp\n     ==========================\n\n  13. Size: 1.00G\n     Operator: op_type=\"Conv2DBackpropInput\" op_name=\"model_5/model_1/conv2d_transpose_3/conv2d_transpose\"\n     Shape: f32[128,128,128,128]{3,0,2,1:T(8,128)}\n     Unpadded size: 1.00G\n     XLA label: fusion.227.remat5 = fusion(get-tuple-element.1095, get-tuple-element.1094, fusion.332.remat3, get-tuple-element.825, ...(+1)), kind=kOutput, calls=fused_computation.197.clone.clone.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  14. Size: 1.00G\n     Operator: op_type=\"Conv2D\" op_name=\"gradient_tape/model_5/model_1/conv2d_transpose_4/conv2d_transpose/Conv2D\"\n     Shape: f32[128,128,128,128]{3,0,2,1:T(8,128)}\n     Unpadded size: 1.00G\n     XLA label: fusion.1260.remat2 = fusion(fusion.227.remat5, get-tuple-element.1097, get-tuple-element.1265, fusion.57.remat7, ...(+6)), kind=kOutput, calls=fused_computation.1106.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  15. Size: 1.00G\n     Operator: op_type=\"Conv2DBackpropInput\" op_name=\"model_5/model_1/conv2d_transpose_3/conv2d_transpose_1\"\n     Shape: f32[128,128,128,128]{3,0,2,1:T(8,128)}\n     Unpadded size: 1.00G\n     XLA label: fusion.1364.remat4 = fusion(get-tuple-element.1094, get-tuple-element.1095, get-tuple-element.1249, get-tuple-element.824, ...(+1)), kind=kOutput, calls=fused_computation.1138.clone.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  16. Size: 1.00G\n     Operator: op_type=\"Conv2D\" op_name=\"gradient_tape/model_5/model_1/conv2d_transpose_4/conv2d_transpose_2/Conv2D\"\n     Shape: f32[128,128,128,128]{3,0,2,1:T(8,128)}\n     Unpadded size: 1.00G\n     XLA label: fusion.1266.remat3 = fusion(get-tuple-element.1267, get-tuple-element.1097, get-tuple-element.1275, get-tuple-element.1246, ...(+6)), kind=kOutput, calls=fused_computation.1112.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  17. Size: 1.00G\n     Operator: op_type=\"Conv2D\" op_name=\"gradient_tape/model_5/model_1/conv2d_transpose_4/conv2d_transpose_1/Conv2D\"\n     Shape: f32[128,128,128,128]{3,0,2,1:T(8,128)}\n     Unpadded size: 1.00G\n     XLA label: fusion.1264.remat2 = fusion(get-tuple-element.1254, get-tuple-element.1097, get-tuple-element.986, fusion.54.remat2.1.remat2, ...(+6)), kind=kOutput, calls=fused_computation.1110.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  18. Size: 1.00G\n     Operator: op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/model_5/model_1/conv2d_5/Conv2D_2/Conv2DBackpropInput\"\n     Shape: f32[128,128,128,128]{3,0,2,1:T(8,128)}\n     Unpadded size: 1.00G\n     XLA label: fusion.1265.remat4 = fusion(get-tuple-element.928, get-tuple-element.1093, get-tuple-element.1307, get-tuple-element.1263, ...(+6)), kind=kOutput, calls=fused_computation.1111.clone.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  19. Size: 1.00G\n     Operator: op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/model_5/model_1/conv2d_5/Conv2D/Conv2DBackpropInput\"\n     Shape: f32[128,128,128,128]{3,0,2,1:T(8,128)}\n     Unpadded size: 1.00G\n     XLA label: fusion.1259.remat2 = fusion(get-tuple-element.926, get-tuple-element.1093, get-tuple-element.1289, fusion.332.remat3, ...(+6)), kind=kOutput, calls=fused_computation.1105.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n  20. Size: 1.00G\n     Operator: op_type=\"Conv2DBackpropInput\" op_name=\"model_5/model_1/conv2d_transpose_3/conv2d_transpose_2\"\n     Shape: f32[128,128,128,128]{3,0,2,1:T(8,128)}\n     Unpadded size: 1.00G\n     XLA label: fusion.1366.remat4 = fusion(get-tuple-element.1094, get-tuple-element.1095, get-tuple-element.1263, get-tuple-element.827, ...(+1)), kind=kOutput, calls=fused_computation.1140.clone.clone.clone.clone\n     Allocation type: HLO temp\n     ==========================\n\n\n2023-09-11 13:19:00.732723: F tensorflow/core/tpu/kernels/tpu_program_group.cc:86] Check failed: xla_tpu_programs.size() > 0 (0 vs. 0)\nhttps://symbolize.stripped_domain/r/?trace=7cbce97f0ccc,7cbce97a1f8f,58a0eba3ddff&map= \n*** SIGABRT received by PID 15 (TID 1124) on cpu 10 from PID 15; stack trace: ***\nPC: @     0x7cbce97f0ccc  (unknown)  (unknown)\n    @     0x7cbc0363605a       1152  (unknown)\n    @     0x7cbce97a1f90      15440  (unknown)\n    @     0x58a0eba3de00  (unknown)  (unknown)\nhttps://symbolize.stripped_domain/r/?trace=7cbce97f0ccc,7cbc03636059,7cbce97a1f8f,58a0eba3ddff&map=1278088d049ad36cb636fbbc76303cb3:7cbbf8000000-7cbc0384d7c0 \nE0911 13:19:00.745211    1124 coredump_hook.cc:414] RAW: Remote crash data gathering hook invoked.\nE0911 13:19:00.745316    1124 client.cc:278] RAW: Coroner client retries enabled (b/136286901), will retry for up to 30 sec.\nE0911 13:19:00.745320    1124 coredump_hook.cc:512] RAW: Sending fingerprint to remote end.\nE0911 13:19:00.745327    1124 coredump_socket.cc:120] RAW: Stat failed errno=2 on socket /var/google/services/logmanagerd/remote_coredump.socket\nE0911 13:19:00.745332    1124 coredump_hook.cc:518] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] Missing crash reporting socket. Is the listener running?\nE0911 13:19:00.745336    1124 coredump_hook.cc:580] RAW: Dumping core locally.\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ninput_image = normalized_pixel_values3[:3]\nfake_images = g_model_PtoM.predict(input_image)\n\n# Display the first few generated images\nnum_images_to_display = 3\n\nfor i in range(num_images_to_display):\n    plt.figure(figsize=(8, 8))\n    # Ensure that pixel values are within the range [-1, 1] for displaying\n    clipped_image = np.clip(fake_images[i], -1, 1)\n    plt.imshow((clipped_image + 1) / 2)  # Normalize to [0, 1] for display\n    plt.axis('off')\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-07T05:08:58.591735Z","iopub.execute_input":"2023-09-07T05:08:58.592235Z","iopub.status.idle":"2023-09-07T05:09:01.167514Z","shell.execute_reply.started":"2023-09-07T05:08:58.592207Z","shell.execute_reply":"2023-09-07T05:09:01.166118Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m input_image \u001b[38;5;241m=\u001b[39m \u001b[43mnormalized_pixel_values3\u001b[49m[:\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m      4\u001b[0m fake_images \u001b[38;5;241m=\u001b[39m g_model_PtoM\u001b[38;5;241m.\u001b[39mpredict(input_image)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Display the first few generated images\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'normalized_pixel_values3' is not defined"],"ename":"NameError","evalue":"name 'normalized_pixel_values3' is not defined","output_type":"error"}]},{"cell_type":"code","source":"from tensorflow import keras\n\n# Load the entire model\nloaded_model = keras.models.load_model('/kaggle/working/discriminator_model_M.h5')\n\n# Print model summary\nloaded_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_model = keras.models.load_model('/kaggle/working/generator_model_PtoM.h5')\n\n# Print model summary\nloaded_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\n\n# Load the entire model\nloaded_model = keras.models.load_model('/kaggle/working/generator_model_PtoM.h5')\n\n# Compile the model manually with the desired optimizer, loss function, and metrics\nloaded_model.compile(\n    optimizer='adam',  # Replace with your desired optimizer\n    loss='mse',        # Replace with your desired loss function\n    metrics=['accuracy']  # Replace with your desired metrics\n)\n\n# Now the model is compiled and ready for use in training or evaluation\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\n# Load the pickled data from the file\nwith open('/kaggle/working/training_history.pkl', 'rb') as file:\n    training_history = pickle.load(file)\n\n# Now you can work with the loaded data\n# For example, you can print the contents of the loaded dictionary\nprint(training_history)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import h5py\n\n# Open the H5 file\nwith h5py.File('/kaggle/working/discriminator_model_M.h5', 'r') as file:\n    # List all the top-level groups/datasets in the file\n    print(\"Top-level items in the H5 file:\")\n    for item in file.keys():\n        print(item)\n\n    # Access and print the contents of a specific dataset\n    dataset = file['model_weights']\n    print(\"Contents of the dataset:\")\n    print(dataset[...])  # Use [()] to access the data\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n    \nprint(tf.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Enable GPU memory growth to allocate only the GPU memory needed.\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        print(e)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strategy = tf.distribute.MirroredStrategy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    normalized_pixel_values2 = pd.read_pickle('/kaggle/input/normalized1/normalized_pixel_values/npv2_data.pkl')\n\n    normalized_pixel_values1 = pd.read_pickle('/kaggle/input/normalized1/npv1_data.pkl/npv1_data.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    normalized_pixel_values3 = pd.read_pickle('/kaggle/input/normalized1/normalized_pixel_values/npv3_data.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    normalized_pixel_values4 = pd.read_pickle('/kaggle/input/normalized1/normalized_pixel_values/npv4_data.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(normalized_pixel_values4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(normalized_pixel_values1[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_generator(data, batch_size):\n    num_samples = len(data)\n    indices = list(range(num_samples))\n    while True:\n        np.random.shuffle(indices)\n        for i in range(0, num_samples, batch_size):\n            batch_indices = indices[i:i+batch_size]\n            batch_data = data.iloc[batch_indices]\n            yield batch_data\nprint('ready')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import GroupNormalization\n# define layer\nlayer = GroupNormalization(axis=-1, groups = -1)\nprint('import successful')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify which GPU to use (e.g., GPU 0)\nwith strategy.scope():\n    # Define your model here\n    # ...\n\n    def discriminator_model(image_shape):\n        init = RandomNormal(stddev = 0.02) #weight initialization with a normal distirbution curve\n        input_image = Input(shape = image_shape)\n        d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(input_image) #64 - number of layers/output channels, (4,4) - size of filter, padding = \"same\" - ensures output has same dimensions as input after padding\n        d = LeakyReLU(alpha=0.2)(d)         \n\n        d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n        d = GroupNormalization(axis=-1, groups = 1)(d)\n        d = LeakyReLU(alpha=0.2)(d)\n\n        d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n        d = GroupNormalization(axis=-1, groups = 1)(d)\n        d = LeakyReLU(alpha=0.2)(d)\n\n        d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n        d = GroupNormalization(axis=-1, groups = 1)(d)\n        d = LeakyReLU(alpha=0.2)(d)\n\n        d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n        d = GroupNormalization(axis=-1, groups = 1)(d)\n        d = LeakyReLU(alpha=0.2)(d)\n\n        patch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n        model = Model(input_image, patch_out)\n\n        model.compile(loss='mse', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), loss_weights=[0.5]) #lr - learning rate, beta_1 - influences moving average of past gradients, loss_weights - shows relative contribution of mse to the overall loss during training\n\n        return model\nprint('discriminator model ready')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify which GPU to use (e.g., GPU 0)\nwith strategy.scope():\n    # Define your model here\n    # ...\n\n    def generator_model(image_shape):\n        init = RandomNormal(stddev = 0.02) #weight initialization with a normal distirbution curve\n        input_image = Input(shape = image_shape)\n        g = Conv2D(64, (7,7), padding = 'same', kernel_initializer = init)(input_image)\n        g = GroupNormalization(axis=-1, groups = 1)(g)\n        g = Activation('relu')(g)\n\n        g =Conv2D(128, (3,3), strides = (2,2), padding = 'same', kernel_initializer = init)(g)\n        g = GroupNormalization(axis=-1, groups = 1)(g)\n        g = Activation('relu')(g)\n\n        g =Conv2D(256, (3,3), strides = (2,2), padding = 'same', kernel_initializer = init)(g)\n        g = GroupNormalization(axis=-1, groups = 1)(g)\n        g = Activation('relu')(g)\n\n        g = Conv2DTranspose(128, (3,3), strides = (2,2), padding = 'same', kernel_initializer = init)(g)\n        g = GroupNormalization(axis=-1, groups = 1)(g)\n        g = Activation('relu')(g)\n\n        g = Conv2DTranspose(64, (3,3), strides = (2,2), padding = 'same', kernel_initializer = init)(g)\n        g = GroupNormalization(axis=-1, groups = 1)(g)\n        g = Activation('relu')(g)\n\n        g = Conv2DTranspose(3, (7,7), padding = 'same', kernel_initializer = init)(g)\n        g = GroupNormalization(axis=-1, groups = 1)(g)\n        output_image = Activation('tanh')(g)\n\n        model = Model(input_image, output_image)\n        return model\nprint('generator model ready')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify which GPU to use (e.g., GPU 0)\nwith strategy.scope():\n    # Define your model here\n    # ...\n\n    def composite_model(g_model_1, d_model, g_model_2, image_shape):\n        g_model_1.trainable = True #we need to change the weights of the main generator\n        d_model.trainable = False\n        g_model_2.trainable = False\n\n        #discriminator element\n        input_gen = Input(shape = image_shape)\n        gen1_out = g_model_1(input_gen)\n        output_d = d_model(gen1_out)\n\n        #identity element\n        input_id = Input(shape = image_shape)\n        output_id = g_model_1(input_id)\n\n        #forward cycle\n        output_f = g_model_2(gen1_out)\n\n        #backward cycle\n        gen2_out = g_model_2(input_id)\n        output_b = g_model_1(gen2_out)\n\n        model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n        optimizer = Adam(learning_rate= 0.0002, beta_1 = 0.5)\n\n        model.compile(loss = ['mse', 'mae', 'mae', 'mae'], loss_weights = [1,5,10,10], optimizer = optimizer)\n        return model \nprint('composite model ready')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n# select a batch of random samples, returns images and target\n    def generate_real_samples(dataset, n_samples, patch_shape):\n     # choose random instances\n     ix = randint(0, dataset.shape[0], n_samples)\n     # retrieve selected images\n     X = dataset[ix]\n     # generate 'real' class labels (1)\n     y = ones((n_samples, patch_shape, patch_shape, 1))\n     return X, y\n    print('real sample model ready')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    # generate a batch of images, returns images and targets\n    def generate_fake_samples(g_model, dataset, patch_shape):\n     # generate fake instance\n     X = g_model.predict(dataset)\n     # create 'fake' class labels (0)\n     y = zeros((len(X), patch_shape, patch_shape, 1))\n     return X, y\nprint('fake sample model ready')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    dataset = [np.concatenate((normalized_pixel_values1, normalized_pixel_values2)), np.concatenate((normalized_pixel_values3, normalized_pixel_values4))]\n    image_shape = (256, 256, 3)\n    g_model_MtoP = generator_model(image_shape)\n    g_model_PtoM = generator_model(image_shape)\n    d_model_M = discriminator_model(image_shape)\n    d_model_P = discriminator_model(image_shape)\n    c_model_MtoP = composite_model(g_model_MtoP, d_model_P, g_model_PtoM, image_shape)\n    c_model_PtoM = composite_model(g_model_PtoM, d_model_M, g_model_MtoP, image_shape)\nprint('ready')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom keras.callbacks import ModelCheckpoint\nwith strategy.scope():\n    # Define hyperparameters\n    num_epochs = 1000  # Adjust as needed\n    batch_size = 1     # You can adjust the batch size\n    patch_shape = 16   # Adjust the patch size as needed\n\n    # Define a directory to save model checkpoints\n    checkpoint_dir = '/kaggle/working/model_checkpoints'  # Adjust the directory as needed\n\n    # Create the directory if it doesn't exist\n    import os\n    os.makedirs(checkpoint_dir, exist_ok=True)\n\n    # Define a file naming pattern for saved models\n    checkpoint_filepath = os.path.join(checkpoint_dir, 'model_epoch_{epoch:03d}.h5')\n\n    # Create a ModelCheckpoint callback to save models\n    model_checkpoint_callback = ModelCheckpoint(\n        filepath=checkpoint_filepath,\n        save_weights_only=True,  # Save only the weights, not the entire model\n        monitor='val_loss',     # Monitor a specific metric (e.g., validation loss)\n        save_best_only=False,   # Save all models or just the best one\n        verbose=1               # Provide progress updates\n    )\n\n    # Training loop\n    # Training loop\n    training_history = {'D_loss_real': [], 'D_loss_fake': [], 'G_loss': []}  # Initialize an empty dictionary to store the training history\n\n    for epoch in range(num_epochs):\n        # Iterate over batches of data (assuming you have a dataset object)\n        for batch in dataset:  # Replace 'your_dataset' with your actual dataset\n            # Train Discriminators\n            real_images, real_labels = generate_real_samples(batch, batch_size, patch_shape)\n            fake_images, fake_labels = generate_fake_samples(g_model_MtoP, batch, patch_shape)\n            d_loss_real = d_model_P.train_on_batch(real_images, real_labels)\n            d_loss_fake = d_model_P.train_on_batch(fake_images, fake_labels)\n\n            # Train Generators\n            real_images, real_labels = generate_real_samples(batch, batch_size, patch_shape)\n            g_loss = c_model_MtoP.train_on_batch([real_images, real_images], [real_labels, real_images, real_images, real_images])\n\n            # Print training progress\n            print(f\"Epoch {epoch+1}/{num_epochs}, D Loss Real: {d_loss_real}, D Loss Fake: {d_loss_fake}, G Loss: {g_loss}\")\n\n        # Save the losses in the training history\n        training_history['D_loss_real'].append(d_loss_real)\n        training_history['D_loss_fake'].append(d_loss_fake)\n        training_history['G_loss'].append(g_loss)\n\n        # Save model checkpoints after each epoch\n        model_checkpoint_callback.on_epoch_end(epoch, logs={'val_loss': g_loss})  # Save the model\n\n\n\n# Finalize training\n# ... (training loop and ModelCheckpoint as mentioned previously)\n\n# Finalize training\n# Save the generator and discriminator models\ng_model_MtoP.save('/kaggle/working/generator_model_MtoP.h5')\n# Save the generator model (you can do the same for the discriminator)\ng_model_PtoM.save('/kaggle/working/generator_model_PtoM.h5')\n# Save the generator model (you can do the same for the discriminator)\nd_model_M.save('/kaggle/working/discriminator_model_M.h5')\n# Save the generator model (you can do the same for the discriminator)\nd_model_P.save('/kaggle/working/discriminator_model_P.h5')\n\n# Save training history (optional)\nimport pickle\nwith open('/kaggle/working/training_history.pkl', 'wb') as history_file:\n    pickle.dump(training_history, history_file)\n\n# Optionally, you can perform additional tasks, such as evaluation or inference\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n","metadata":{},"execution_count":null,"outputs":[]}]}