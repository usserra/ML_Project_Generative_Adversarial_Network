{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Cells Seperated"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from random import random\n","from numpy import load\n","from numpy import zeros\n","from numpy import ones\n","from numpy import asarray\n","from numpy.random import randint\n","from keras.optimizers import Adam\n","from keras.initializers import RandomNormal\n","from keras.models import Model\n","from tensorflow.keras import Input\n","from keras.layers import Conv2D\n","from keras.layers import Conv2DTranspose\n","from keras.layers import LeakyReLU\n","from keras.layers import Activation\n","from keras.layers import Concatenate\n","import matplotlib.pyplot as plt\n","from os import listdir\n","#from keras.preprocessing.image import load_img\n","from numpy import savez_compressed\n","from keras import backend as K\n","import os\n","import numpy as np\n","from PIL import Image\n","import pandas as pd\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=\"local\")\n","    strategy = tf.distribute.TPUStrategy(tpu)\n","    print(\"on TPU\")\n","except tf.errors.NotFoundError:\n","    print(\"not on TPU\")\n","    strategy = tf.distribute.MirroredStrategy()\n","\n","    print(\"REPLICAS: \", strategy.num_replicas_in_sync)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with strategy.scope():\n","    normalized_pixel_values2 = pd.read_pickle('/kaggle/input/normalized1/normalized_pixel_values/npv2_data.pkl')\n","\n","    normalized_pixel_values1 = pd.read_pickle('/kaggle/input/normalized1/npv1_data.pkl/npv1_data.pkl')\n","    \n","    normalized_pixel_values3 = pd.read_pickle('/kaggle/input/normalized1/normalized_pixel_values/npv3_data.pkl')\n","        \n","    normalized_pixel_values4 = pd.read_pickle('/kaggle/input/normalized1/normalized_pixel_values/npv4_data.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import GroupNormalization\n","# define layer\n","layer = GroupNormalization(axis=-1, groups = -1)\n","print('import successful')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def discriminator_model(image_shape):\n","        init = RandomNormal(stddev = 0.02) #weight initialization with a normal distirbution curve\n","        input_image = Input(shape = image_shape)\n","        d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(input_image) #64 - number of layers/output channels, (4,4) - size of filter, padding = \"same\" - ensures output has same dimensions as input after padding\n","        d = LeakyReLU(alpha=0.2)(d)         \n","\n","        d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n","        d = GroupNormalization(axis=-1, groups = 1)(d)\n","        d = LeakyReLU(alpha=0.2)(d)\n","\n","        d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n","        d = GroupNormalization(axis=-1, groups = 1)(d)\n","        d = LeakyReLU(alpha=0.2)(d)\n","\n","        d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n","        d = GroupNormalization(axis=-1, groups = 1)(d)\n","        d = LeakyReLU(alpha=0.2)(d)\n","\n","        d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n","        d = GroupNormalization(axis=-1, groups = 1)(d)\n","        d = LeakyReLU(alpha=0.2)(d)\n","\n","        patch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n","        model = Model(input_image, patch_out)\n","\n","        model.compile(loss='mse', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), loss_weights=[0.5], metrics=['accuracy']) #lr - learning rate, beta_1 - influences moving average of past gradients, loss_weights - shows relative contribution of mse to the overall loss during training\n","        print('discriminator model ready')\n","        return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generator_model(image_shape):\n","        init = RandomNormal(stddev = 0.02) #weight initialization with a normal distirbution curve\n","        input_image = Input(shape = image_shape)\n","        g = Conv2D(64, (7,7), padding = 'same', kernel_initializer = init)(input_image) #64 - number of filters, (7 x 7) - size of filter\n","        g = GroupNormalization(axis=-1, groups = 1)(g)\n","        g = Activation('relu')(g)\n","\n","        g =Conv2D(128, (3,3), strides = (2,2), padding = 'same', kernel_initializer = init)(g)\n","        g = GroupNormalization(axis=-1, groups = 1)(g)\n","        g = Activation('relu')(g)\n","\n","        g =Conv2D(256, (3,3), strides = (2,2), padding = 'same', kernel_initializer = init)(g)\n","        g = GroupNormalization(axis=-1, groups = 1)(g)\n","        g = Activation('relu')(g)\n","\n","        g = Conv2DTranspose(128, (3,3), strides = (2,2), padding = 'same', kernel_initializer = init)(g)\n","        g = GroupNormalization(axis=-1, groups = 1)(g)\n","        g = Activation('relu')(g)\n","\n","        g = Conv2DTranspose(64, (3,3), strides = (2,2), padding = 'same', kernel_initializer = init)(g)\n","        g = GroupNormalization(axis=-1, groups = 1)(g)\n","        g = Activation('relu')(g)\n","\n","        g = Conv2DTranspose(3, (7,7), padding = 'same', kernel_initializer = init)(g)\n","        g = GroupNormalization(axis=-1, groups = 1)(g)\n","        output_image = Activation('tanh')(g)\n","        \n","        model = Model(input_image, output_image)\n","        print('generator model ready')\n","        return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def composite_model(g_model_1, d_model, g_model_2, image_shape):\n","        g_model_1.trainable = True #we need to change the weights of the main generator\n","        d_model.trainable = False\n","        g_model_2.trainable = False\n","\n","        #discriminator element\n","        input_gen = Input(shape = image_shape)\n","        gen1_out = g_model_1(input_gen)\n","        output_d = d_model(gen1_out)\n","\n","        #identity element\n","        input_id = Input(shape = image_shape)\n","        output_id = g_model_1(input_id)\n","\n","        #forward cycle\n","        output_f = g_model_2(gen1_out)\n","\n","        #backward cycle\n","        gen2_out = g_model_2(input_id)\n","        output_b = g_model_1(gen2_out)\n","\n","        model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n","        optimizer = Adam(learning_rate= 0.0002, beta_1 = 0.5)\n","\n","        model.compile(loss = ['mse', 'mae', 'mae', 'mae'], loss_weights = [1,5,10,10], optimizer = optimizer, metrics=['accuracy'])\n","        print('composite model ready')\n","        return model "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_real_samples(dataset, n_samples, patch_shape):\n","         # choose random instances\n","        ix = randint(0, dataset.shape[0], n_samples)\n","         # retrieve selected images\n","        X = dataset[ix]\n","         # generate 'real' class labels (1)\n","        y = ones((n_samples, patch_shape, patch_shape, 1))\n","        print('real sample model ready')\n","        return X, y\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_fake_samples(g_model, dataset, patch_shape):\n","     # generate fake instance\n","        X = g_model.predict(dataset)\n","     # create 'fake' class labels (0)\n","        y = zeros((len(X), patch_shape, patch_shape, 1))\n","        print('fake sample model ready')\n","        return X, y\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with strategy.scope():\n","    dataset = [np.concatenate((normalized_pixel_values1, normalized_pixel_values2)), np.concatenate((normalized_pixel_values3, normalized_pixel_values4))]\n","    dataM, dataP = dataset\n","    image_shape = (256, 256, 3)\n","    g_model_MtoP = generator_model(image_shape)\n","    g_model_PtoM = generator_model(image_shape)\n","    d_model_M = discriminator_model(image_shape)\n","    d_model_P = discriminator_model(image_shape)\n","    c_model_MtoP = composite_model(g_model_MtoP, d_model_P, g_model_PtoM, image_shape)\n","    c_model_PtoM = composite_model(g_model_PtoM, d_model_M, g_model_MtoP, image_shape)\n","print('ready')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from keras.callbacks import ModelCheckpoint\n","from keras.models import load_model\n","with strategy.scope():\n","        # Define hyperparameters\n","    num_epochs = 2  # Adjust as needed\n","    batch_size = 1     # You can adjust the batch size\n","    patch_shape = d_model_M.output_shape[1]   # Adjust the patch size as needed\n","\n","        # Define a directory to save model checkpoints\n","    checkpoint_dir = '/kaggle/working/model_checkpoints'  # Adjust the directory as needed\n","\n","        # Create the directory if it doesn't exist\n","    import os\n","    os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","        # Define a file naming pattern for saved models\n","    checkpoint_filepath = os.path.join(checkpoint_dir, 'model_epoch_{epoch:03d}.h5')\n","\n","        # Create a ModelCheckpoint callback to save models\n","    model_checkpoint_callback = ModelCheckpoint(\n","        filepath=checkpoint_filepath,\n","        save_weights_only=True,  # Save only the weights, not the entire model\n","        monitor='val_loss',     # Monitor a specific metric (e.g., validation loss)\n","        save_best_only=False,   # Save all models or just the best one\n","        verbose=1, # Provide progress updates\n","\n","        )\n","\n","        # Training loop\n","        # Training loop\n","    training_history = {'D_loss_realM': [], 'D_loss_realP': [], 'D_loss_fakeM': [], 'D_loss_fakeP': [], 'G_lossM': [], 'G_lossP': []}  # Initialize an empty dictionary to store the training history\n","\n","    bat_per_epo = int(len(dataM) / batch_size)\n","     # calculate the number of training iterations\n","    n_steps = bat_per_epo * num_epochs\n","     # manually enumerate epochs\n","    for epoch in range(n_steps):\n","                # Train Discriminators\n","        real_imagesM, real_labelsM = generate_real_samples(dataM, batch_size, patch_shape) #This line generates a batch of real images from the current batch in the dataset and assigns them real labels (usually 1, indicating real images).\n","        real_imagesP, real_labelsP = generate_real_samples(dataP, batch_size, patch_shape)\n","\n","        fake_imagesM, fake_labelsM = generate_fake_samples(g_model_PtoM, real_imagesP, patch_shape) #: This line generates a batch of fake images by passing a batch from the dataset through the generator g_model_MtoP and assigns them fake labels (usually 0, indicating fake images).\n","        fake_imagesP, fake_labelsP = generate_fake_samples(g_model_MtoP, real_imagesM, patch_shape)\n","\n","        g_lossP = c_model_PtoM.train_on_batch([real_imagesP, real_imagesM], [real_labelsM, real_imagesM, real_imagesP, real_imagesM])\n","\n","        d_loss_realM = d_model_M.train_on_batch(real_imagesM, real_labelsM)\n","        d_loss_fakeM = d_model_M.train_on_batch(fake_imagesM, fake_labelsM)\n","\n","        g_lossM = c_model_MtoP.train_on_batch([real_imagesM, real_imagesP], [real_labelsP, real_imagesP, real_imagesM, real_imagesP])\n","\n","        d_loss_realP = d_model_P.train_on_batch(real_imagesP, real_labelsP)\n","        d_loss_fakeP = d_model_P.train_on_batch(fake_imagesP, fake_labelsP)\n","\n","        # Print training progress\n","        print(f\"Epoch {epoch+1}/{n_steps}, D Loss RealM: {d_loss_realM}, D Loss RealP: {d_loss_realP}, D Loss FakeM: {d_loss_fakeM}, D Loss FakeP: {d_loss_fakeP}, G LossM: {g_lossM}, G LossP: {g_lossP}\")\n","        training_history['D_loss_realM'].append(d_loss_realM)\n","        training_history['D_loss_realP'].append(d_loss_realP)\n","        training_history['D_loss_fakeM'].append(d_loss_fakeM)\n","        training_history['D_loss_fakeP'].append(d_loss_fakeP)\n","        training_history['G_lossM'].append(g_lossM)\n","        training_history['G_lossM'].append(g_lossP)\n","\n","\n","\n","    g_model_MtoP.save('/kaggle/working/generator_model_MtoP.h5')\n","    # Save the generator model (you can do the same for the discriminator)\n","    g_model_PtoM.save('/kaggle/working/generator_model_PtoM.h5')\n","    # Save the generator model (you can do the same for the discriminator)\n","    d_model_M.save('/kaggle/working/discriminator_model_M.h5')\n","    # Save the generator model (you can do the same for the discriminator)\n","    d_model_P.save('/kaggle/working/discriminator_model_P.h5')\n","\n","    # Save training history (optional)\n","    import pickle\n","    with open('/kaggle/working/training_history.pkl', 'wb') as history_file:\n","        pickle.dump(training_history, history_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","input_image = normalized_pixel_values3[:3]\n","fake_images = g_model_PtoM.predict(input_image)\n","\n","# Display the first few generated images\n","num_images_to_display = 3\n","\n","for i in range(num_images_to_display):\n","    plt.figure(figsize=(8, 8))\n","    # Ensure that pixel values are within the range [-1, 1] for displaying\n","    clipped_image = np.clip(fake_images[i], -1, 1)\n","    plt.imshow((clipped_image + 1) / 2)  # Normalize to [0, 1] for display\n","    plt.axis('off')\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow import keras\n","\n","# Load the entire model\n","loaded_model = keras.models.load_model('/kaggle/working/discriminator_model_M.h5')\n","\n","# Print model summary\n","loaded_model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["loaded_model = keras.models.load_model('/kaggle/working/generator_model_PtoM.h5')\n","\n","# Print model summary\n","loaded_model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow import keras\n","\n","# Load the entire model\n","loaded_model = keras.models.load_model('/kaggle/working/generator_model_PtoM.h5')\n","\n","# Compile the model manually with the desired optimizer, loss function, and metrics\n","loaded_model.compile(\n","    optimizer='adam',  # Replace with your desired optimizer\n","    loss='mse',        # Replace with your desired loss function\n","    metrics=['accuracy']  # Replace with your desired metrics\n",")\n","\n","# Now the model is compiled and ready for use in training or evaluation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pickle\n","\n","# Load the pickled data from the file\n","with open('/kaggle/working/training_history.pkl', 'rb') as file:\n","    training_history = pickle.load(file)\n","\n","# Now you can work with the loaded data\n","# For example, you can print the contents of the loaded dictionary\n","print(training_history)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import h5py\n","\n","# Open the H5 file\n","with h5py.File('/kaggle/working/discriminator_model_M.h5', 'r') as file:\n","    # List all the top-level groups/datasets in the file\n","    print(\"Top-level items in the H5 file:\")\n","    for item in file.keys():\n","        print(item)\n","\n","    # Access and print the contents of a specific dataset\n","    dataset = file['model_weights']\n","    print(\"Contents of the dataset:\")\n","    print(dataset[...])  # Use [()] to access the data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    print('Device:', tpu.master())\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","except:\n","    strategy = tf.distribute.get_strategy()\n","print('Number of replicas:', strategy.num_replicas_in_sync)\n","\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","    \n","print(tf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        # Enable GPU memory growth to allocate only the GPU memory needed.\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","    except RuntimeError as e:\n","        print(e)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["strategy = tf.distribute.MirroredStrategy()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with strategy.scope():\n","    normalized_pixel_values2 = pd.read_pickle('/kaggle/input/normalized1/normalized_pixel_values/npv2_data.pkl')\n","\n","    normalized_pixel_values1 = pd.read_pickle('/kaggle/input/normalized1/npv1_data.pkl/npv1_data.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with strategy.scope():\n","    normalized_pixel_values3 = pd.read_pickle('/kaggle/input/normalized1/normalized_pixel_values/npv3_data.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with strategy.scope():\n","    normalized_pixel_values4 = pd.read_pickle('/kaggle/input/normalized1/normalized_pixel_values/npv4_data.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(normalized_pixel_values4)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(normalized_pixel_values1[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def data_generator(data, batch_size):\n","    num_samples = len(data)\n","    indices = list(range(num_samples))\n","    while True:\n","        np.random.shuffle(indices)\n","        for i in range(0, num_samples, batch_size):\n","            batch_indices = indices[i:i+batch_size]\n","            batch_data = data.iloc[batch_indices]\n","            yield batch_data\n","print('ready')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import GroupNormalization\n","# define layer\n","layer = GroupNormalization(axis=-1, groups = -1)\n","print('import successful')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Specify which GPU to use (e.g., GPU 0)\n","with strategy.scope():\n","    # Define your model here\n","    # ...\n","\n","    def discriminator_model(image_shape):\n","        init = RandomNormal(stddev = 0.02) #weight initialization with a normal distirbution curve\n","        input_image = Input(shape = image_shape)\n","        d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(input_image) #64 - number of layers/output channels, (4,4) - size of filter, padding = \"same\" - ensures output has same dimensions as input after padding\n","        d = LeakyReLU(alpha=0.2)(d)         \n","\n","        d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n","        d = GroupNormalization(axis=-1, groups = 1)(d)\n","        d = LeakyReLU(alpha=0.2)(d)\n","\n","        d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n","        d = GroupNormalization(axis=-1, groups = 1)(d)\n","        d = LeakyReLU(alpha=0.2)(d)\n","\n","        d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n","        d = GroupNormalization(axis=-1, groups = 1)(d)\n","        d = LeakyReLU(alpha=0.2)(d)\n","\n","        d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n","        d = GroupNormalization(axis=-1, groups = 1)(d)\n","        d = LeakyReLU(alpha=0.2)(d)\n","\n","        patch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n","        model = Model(input_image, patch_out)\n","\n","        model.compile(loss='mse', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), loss_weights=[0.5]) #lr - learning rate, beta_1 - influences moving average of past gradients, loss_weights - shows relative contribution of mse to the overall loss during training\n","\n","        return model\n","print('discriminator model ready')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Specify which GPU to use (e.g., GPU 0)\n","with strategy.scope():\n","    # Define your model here\n","    # ...\n","\n","    def generator_model(image_shape):\n","        init = RandomNormal(stddev = 0.02) #weight initialization with a normal distirbution curve\n","        input_image = Input(shape = image_shape)\n","        g = Conv2D(64, (7,7), padding = 'same', kernel_initializer = init)(input_image)\n","        g = GroupNormalization(axis=-1, groups = 1)(g)\n","        g = Activation('relu')(g)\n","\n","        g =Conv2D(128, (3,3), strides = (2,2), padding = 'same', kernel_initializer = init)(g)\n","        g = GroupNormalization(axis=-1, groups = 1)(g)\n","        g = Activation('relu')(g)\n","\n","        g =Conv2D(256, (3,3), strides = (2,2), padding = 'same', kernel_initializer = init)(g)\n","        g = GroupNormalization(axis=-1, groups = 1)(g)\n","        g = Activation('relu')(g)\n","\n","        g = Conv2DTranspose(128, (3,3), strides = (2,2), padding = 'same', kernel_initializer = init)(g)\n","        g = GroupNormalization(axis=-1, groups = 1)(g)\n","        g = Activation('relu')(g)\n","\n","        g = Conv2DTranspose(64, (3,3), strides = (2,2), padding = 'same', kernel_initializer = init)(g)\n","        g = GroupNormalization(axis=-1, groups = 1)(g)\n","        g = Activation('relu')(g)\n","\n","        g = Conv2DTranspose(3, (7,7), padding = 'same', kernel_initializer = init)(g)\n","        g = GroupNormalization(axis=-1, groups = 1)(g)\n","        output_image = Activation('tanh')(g)\n","\n","        model = Model(input_image, output_image)\n","        return model\n","print('generator model ready')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Specify which GPU to use (e.g., GPU 0)\n","with strategy.scope():\n","    # Define your model here\n","    # ...\n","\n","    def composite_model(g_model_1, d_model, g_model_2, image_shape):\n","        g_model_1.trainable = True #we need to change the weights of the main generator\n","        d_model.trainable = False\n","        g_model_2.trainable = False\n","\n","        #discriminator element\n","        input_gen = Input(shape = image_shape)\n","        gen1_out = g_model_1(input_gen)\n","        output_d = d_model(gen1_out)\n","\n","        #identity element\n","        input_id = Input(shape = image_shape)\n","        output_id = g_model_1(input_id)\n","\n","        #forward cycle\n","        output_f = g_model_2(gen1_out)\n","\n","        #backward cycle\n","        gen2_out = g_model_2(input_id)\n","        output_b = g_model_1(gen2_out)\n","\n","        model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n","        optimizer = Adam(learning_rate= 0.0002, beta_1 = 0.5)\n","\n","        model.compile(loss = ['mse', 'mae', 'mae', 'mae'], loss_weights = [1,5,10,10], optimizer = optimizer)\n","        return model \n","print('composite model ready')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with strategy.scope():\n","# select a batch of random samples, returns images and target\n","    def generate_real_samples(dataset, n_samples, patch_shape):\n","     # choose random instances\n","     ix = randint(0, dataset.shape[0], n_samples)\n","     # retrieve selected images\n","     X = dataset[ix]\n","     # generate 'real' class labels (1)\n","     y = ones((n_samples, patch_shape, patch_shape, 1))\n","     return X, y\n","    print('real sample model ready')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with strategy.scope():\n","    # generate a batch of images, returns images and targets\n","    def generate_fake_samples(g_model, dataset, patch_shape):\n","     # generate fake instance\n","     X = g_model.predict(dataset)\n","     # create 'fake' class labels (0)\n","     y = zeros((len(X), patch_shape, patch_shape, 1))\n","     return X, y\n","print('fake sample model ready')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with strategy.scope():\n","    dataset = [np.concatenate((normalized_pixel_values1, normalized_pixel_values2)), np.concatenate((normalized_pixel_values3, normalized_pixel_values4))]\n","    image_shape = (256, 256, 3)\n","    g_model_MtoP = generator_model(image_shape)\n","    g_model_PtoM = generator_model(image_shape)\n","    d_model_M = discriminator_model(image_shape)\n","    d_model_P = discriminator_model(image_shape)\n","    c_model_MtoP = composite_model(g_model_MtoP, d_model_P, g_model_PtoM, image_shape)\n","    c_model_PtoM = composite_model(g_model_PtoM, d_model_M, g_model_MtoP, image_shape)\n","print('ready')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","from keras.callbacks import ModelCheckpoint\n","with strategy.scope():\n","    # Define hyperparameters\n","    num_epochs = 1000  # Adjust as needed\n","    batch_size = 1     # You can adjust the batch size\n","    patch_shape = 16   # Adjust the patch size as needed\n","\n","    # Define a directory to save model checkpoints\n","    checkpoint_dir = '/kaggle/working/model_checkpoints'  # Adjust the directory as needed\n","\n","    # Create the directory if it doesn't exist\n","    import os\n","    os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","    # Define a file naming pattern for saved models\n","    checkpoint_filepath = os.path.join(checkpoint_dir, 'model_epoch_{epoch:03d}.h5')\n","\n","    # Create a ModelCheckpoint callback to save models\n","    model_checkpoint_callback = ModelCheckpoint(\n","        filepath=checkpoint_filepath,\n","        save_weights_only=True,  # Save only the weights, not the entire model\n","        monitor='val_loss',     # Monitor a specific metric (e.g., validation loss)\n","        save_best_only=False,   # Save all models or just the best one\n","        verbose=1               # Provide progress updates\n","    )\n","\n","    # Training loop\n","    # Training loop\n","    training_history = {'D_loss_real': [], 'D_loss_fake': [], 'G_loss': []}  # Initialize an empty dictionary to store the training history\n","\n","    for epoch in range(num_epochs):\n","        # Iterate over batches of data (assuming you have a dataset object)\n","        for batch in dataset:  # Replace 'your_dataset' with your actual dataset\n","            # Train Discriminators\n","            real_images, real_labels = generate_real_samples(batch, batch_size, patch_shape)\n","            fake_images, fake_labels = generate_fake_samples(g_model_MtoP, batch, patch_shape)\n","            d_loss_real = d_model_P.train_on_batch(real_images, real_labels)\n","            d_loss_fake = d_model_P.train_on_batch(fake_images, fake_labels)\n","\n","            # Train Generators\n","            real_images, real_labels = generate_real_samples(batch, batch_size, patch_shape)\n","            g_loss = c_model_MtoP.train_on_batch([real_images, real_images], [real_labels, real_images, real_images, real_images])\n","\n","            # Print training progress\n","            print(f\"Epoch {epoch+1}/{num_epochs}, D Loss Real: {d_loss_real}, D Loss Fake: {d_loss_fake}, G Loss: {g_loss}\")\n","\n","        # Save the losses in the training history\n","        training_history['D_loss_real'].append(d_loss_real)\n","        training_history['D_loss_fake'].append(d_loss_fake)\n","        training_history['G_loss'].append(g_loss)\n","\n","        # Save model checkpoints after each epoch\n","        model_checkpoint_callback.on_epoch_end(epoch, logs={'val_loss': g_loss})  # Save the model\n","\n","\n","\n","# Finalize training\n","# ... (training loop and ModelCheckpoint as mentioned previously)\n","\n","# Finalize training\n","# Save the generator and discriminator models\n","g_model_MtoP.save('/kaggle/working/generator_model_MtoP.h5')\n","# Save the generator model (you can do the same for the discriminator)\n","g_model_PtoM.save('/kaggle/working/generator_model_PtoM.h5')\n","# Save the generator model (you can do the same for the discriminator)\n","d_model_M.save('/kaggle/working/discriminator_model_M.h5')\n","# Save the generator model (you can do the same for the discriminator)\n","d_model_P.save('/kaggle/working/discriminator_model_P.h5')\n","\n","# Save training history (optional)\n","import pickle\n","with open('/kaggle/working/training_history.pkl', 'wb') as history_file:\n","    pickle.dump(training_history, history_file)\n","\n","# Optionally, you can perform additional tasks, such as evaluation or inference\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
